\documentclass[../../cs-notes.tex]{subfiles}

\title{Machine learning}
\author{Benjamin Boboul}
\date{\today}

\begin{document}
	\maketitle

	Ce document concat\`ene un ensemble de notes diverses portant sur le \textit{Machine Learning}.
	C'est une manière pour moi de m'engager dans l'apprentissage de ce domaine et de garder une trace de mes progrès.
	Si ce document se r\'ev\`ele vous être utile ou que vous remarquez des erreurs de ma part, merci de me communiquer toutes remarques.
	Contact: benjamin.boboul@imt-lille-douai.org

	\part{Feature engineering \& preprocessing}

	\paragraph{StandarScaler} \textit{Scikit-learn} propose diverses fonction de prétraitement, \textit{StandarScaler} s'assure que les échantillons aient une moyenne nulle et une variance de 1.

	\paragraph{RobustScaler} Similaire à \textit{StandarScaler}, 

	\paragraph{PCA} L'analyse en composantes principales (ACP) consiste à calculer les composantes principales et à les utiliser pour effectuer un changement de base sur les données, en utilisant parfois uniquement les quelques premières composantes principales et en ignorant le reste. 
	L'ACP est utilisée dans l'analyse exploratoire des données et pour élaborer des modèles prédictifs.
	Elle est couramment utilisée pour la réduction de la dimensionnalité en projetant chaque point de données sur seulement les quelques premières composantes principales afin d'obtenir des données de plus faible dimension tout en préservant autant que possible la variation des données.

	Pour ce faire, l'ACP s'intéresse à la corrélation entre les caractéristiques. 
	Si la corrélation est très élevée parmi un sous-ensemble de caractéristiques, l'ACP tentera de combiner les caractéristiques hautement corrélées et de représenter ces données avec un plus petit nombre de caractéristiques linéairement non corrélées.
	L'algorithme continue d'effectuer cette réduction de corrélation, en trouvant les directions de variance maximale dans les données originales à haute dimension et en les projetant sur un espace de plus petite dimension.
	Ces composantes nouvellement dérivées sont connues sous le nom de composantes principales.

	\paragraph{Incremental PCA} Pour les ensembles de données très volumineux qui ne peuvent pas tenir en mémoire, nous pouvons effectuer l'ACP de manière incrémentale par petits lots, où chaque lot peut tenir en mémoire. La taille du lot peut être définie manuellement ou déterminée automatiquement. Cette forme d'ACP basée sur les lots est connue sous le nom d'ACP incrémentale.

	\paragraph{Sparce PCA} L'algorithme normal de l'ACP recherche des combinaisons linéaires dans toutes les variables d'entrée, réduisant l'espace de caractéristiques original aussi dense que possible.
	Mais pour certains problèmes d'apprentissage automatique, un certain degré de dispersion peut être préféré.
	Une version de l'ACP qui conserve un certain degré de dispersion, contrôlé par un hyperparamètre appelé alpha \(\alpha\), est connue sous le nom d'ACP éparse.
	L'algorithme PCA éparse recherche des combinaisons linéaires dans seulement certaines des variables d'entrée, réduisant l'espace caractéristique original jusqu'à un certain degré, mais pas de manière aussi compacte que l'ACP normale.

	\paragraph{Kernel PCA}
	L'ACP normale, l'ACP incrémentale et l'ACP éparse projettent linéairement les données d'origine dans un espace de dimension inférieure, mais il existe également une forme non linéaire d'ACP, appelée ACP à noyau, qui exécute une fonction de similarité sur des paires de points de données d'origine afin d'effectuer une réduction non linéaire de la dimension.

	\part{Supervised learning}

	\input{notes/machine-learning/introduction.tex}
	\input{notes/machine-learning/linear-regression/linear-regression}
	\input{notes/machine-learning/logistic-regression/logistic-regression}
	\input{notes/machine-learning/knn/knn}
	\input{notes/machine-learning/svm/svm}
	\input{notes/machine-learning/tree-based-methods/decision-tree}
	\input{notes/machine-learning/tree-based-methods/random-forests}
	\input{notes/machine-learning/ann/ann}
	\input{notes/machine-learning/cnn/cnn}
	\input{notes/machine-learning/rnn/rnn}

	\part{Unsupervised learning}

	À la différence de l'apprentissage supervisé, nos données ne disposent pas de labels.
	Impossible donc d'entraîner un algorithme en fonction d'une valeur attendue, cette solution permet cependant d'identifier des motifs dans notre jeu de données.

	\paragraph{Application d'apprentissage non-supervisé}
	Parmis les applications d'apprentissage non-supervisé, nous allons dénoter une utilisation plus fréquente dans le cadre de la détection d'anomalies et la segmentation en groupe.

	\subparagraph{Anomaly Detection} Les système de détections d'anomalies sont utilisé pour repérer des fraudes ou des évènements qui dénotent d'une certaine tendance.

	\subparagraph{Group segmentation}
	Les algorithmes de segmentation sont utilisé pour classifier du contenu, des données en fonction de leurs similarités.

	\chapter{Réduction de dimensions}
	\section{Projection linéaire}
	\section{Manifold Learning}


	\chapter{Clustering}
	Utiliser un algorithme de \textit{clustering} vise à identifier différentes classes d'un jeu de données.
	Pour ce faire, on identifie plusieurs sous-ensembles que l'on groupe en fonction de leurs similarités.

	\section{K-means}


	\section{DBSCAN}
	\textsc{dbscan} utilise un paramètre \textit{min\_samples} pour identifier les sous-ensembles de taille suffisante.
	Si deux sous-ensembles sont à proximité, celui dont la densité est la plus grande l'emporte sur le second.

	\inputminted{python}{code/machine-learning/unsupervised-learning/dbscan.py}

	\chapter{Feature Extraction}

	\section{Autoencoders}

	\input{notes/machine-learning/unsupervised/SOM/SOM}


\end{document}
