\gls{SVM} est un algorithme de \textit{machine learning} utilisé pour la classification et la prédiction de données\footnote{Nous dénoterons cependant une utilisation plus fréquente dans des problèmes de classification.}. 
Dans ce chapitre nous verrons le fonctionnement de cet algorithme dans le cadre d'une classification linéairement séparable et non-linéairement séparable.

\paragraph{}

Dans un objectif de classification, l'utilisation de \gls{SVM} se résume à trouver un hyperplan dans un environnement à $N$ dimensions optimisé pour classification de nouvelles données.
En d'autres termes, nous souhaitons trouver un hyperplan dont la distance aux aux éléments les plus proches est la plus grande.

\paragraph{Applications}
Nous retrouvons diverses utilisations de \gls{SVM} dans la résolutions de problèmes tels que : 
\begin{itemize}
	\item \gls{SVM}s are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.
	\item Classification of images
	\item Classification of satellite data
	\item Hand-written characters can be recognized using SVM
	\item Proteins classification
\end{itemize}

\section{Introduction}

Pour commencer nous parlerons de classification binaire (classe 0 et classe 1). Considérons un vecteur de poids $\mathbf w = \langle \begin{matrix} w_1 & \dots & w_N \end{matrix} \rangle^T$, un vecteur d'entrée $\mathbf x = \langle \begin{matrix} x_1 & \dots & x_N \end{matrix} \rangle^T$ ainsi qu'un biais $b$ qui se révèle être un nombre réel.
From this equation, we can determine whether or not a new entry vector $\mathbf x$ belongs to which class.
If $h(\mathbf x) \geq 0$ then $\mathbf x$ is marked as positive and vice versa:

\begin{equation}
	h(\mathbf x) = \mathbf w^T\mathbf x + b \begin{cases}h(\mathbf x)\geq 0\to \text{class 1} \\h(\mathbf x)< 0\to \text{class 2}\end{cases}
\end{equation}

\subsection{Train SVMs}

An hyperplane try to maximize margin, in other terms, the minimal distance between training vectors and hyperplane.
Thoses training vectors located at this minimal distance are called \textit{support vectors}.

\paragraph{Compute margin}

Given a vector $\mathbf x_k$, we can compute its distance with $h_{w,b}$ :

\begin{equation}
	\frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

where $\lVert \mathbf w \rVert$ is the euclidian distance, the general formula regardless of the size of the dimensions ($N$) is given by:
\begin{equation}
	\lVert \mathbf w \rVert = \sqrt{\sum_{i=1}^N x_i^2}
\end{equation}

The margin of an hyperplane is the smallest distance to one of the support vector and it's obtain by computing the following formula:

\begin{equation}
	\min_k \frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

Since we're looking to maximize margin, \textit{find the hyperplane with the largest margin}, we'll keep the maximum value in functions of $w$ and $b$ with the following equation:

\begin{equation}
	\arg\max_{w,b}\min_k \frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

\begin{equation}
	\min_{w,b,\xi} \frac{1}{2} \lVert \mathbf w \rVert ^2 + C\sum_{i=1}^m \xi_i^p
\end{equation}


\section{Kernel trick}

Here is a non-exhaustive list of different kernels available to our case:
\begin{description}
	\item[Polynomial] $K(x, x\prime) = (\alpha x^T x\prime + \lambda )^d$
	\item[Gaussien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime \rVert^2}{2\sigma^2})$
	\item[Laplacien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime\rVert}{\sigma}$
	\item[Rationnel] $K(x, x\prime) = 1 - \frac{\lVert x\prime - x \rVert^2}{\lVert x\prime - x \rVert^2 + \sigma}$
\end{description}


\section{Python implementation}

\subsection{Classification}

\inputminted{python}{code/machine-learning/svc.py}

\subsection{Regression}

\inputminted{python}{code/machine-learning/svr.py}

