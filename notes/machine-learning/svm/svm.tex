\chapter{Support Vector Machines}

\gls{SVM} est un algorithme de \textit{machine learning} utilisé pour la classification et la prédiction de données\footnote{Nous dénoterons cependant une utilisation plus fréquente dans des problèmes de classification.}.
Dans ce chapitre nous verrons le fonctionnement de cet algorithme dans le cadre d'une classification linéairement séparable et non-linéairement séparable.

\paragraph{}

Dans un objectif de classification, l'utilisation de \gls{SVM} se résume à trouver un hyperplan dans un environnement à $N$ dimensions optimisé pour classification de nouvelles données.
En d'autres termes, nous souhaitons trouver un hyperplan dont la distance aux aux éléments les plus proches est la plus grande.

\paragraph{Applications}
Nous retrouvons diverses utilisations de \gls{SVM} dans la résolutions de problèmes tels que : 
\begin{itemize}
	\item Classification d'images,
	\item Classification de donn\'ees satellites,
	\item identification de lettres manuscrites,
	\item classification de prot\'eines.
\end{itemize}

\section{Introduction}

Pour commencer nous parlerons de classification binaire (classe 0 et classe 1). Considérons un vecteur de poids $\vec w = \langle \begin{matrix} w_1 & \dots & w_N \end{matrix} \rangle^T$, un vecteur d'entrée $\vec x = \langle \begin{matrix} x_1 & \dots & x_N \end{matrix} \rangle^T$ ainsi qu'un biais $b$ qui se révèle être un nombre réel.
From this equation, we can determine whether or not a new entry vector $\vec x$ belongs to which class.
If $h(\mathbf x) \geq 0$ then $\vec x$ is marked as positive and vice versa:

\begin{equation}
	h(\vec x) = \vec w^T\vec x + b
	\begin{cases}
		h(\vec x)\geq 0\to \text{class 1} \\
		h(\vec x)< 0\to \text{class 2}
	\end{cases}
\end{equation}

\subsection{Train \gls{SVM}s}\label{subsec:train-svm}

En utilisant des \gls{SVM}s, nous cherchons un hyperplan qui maximize les marges des vecteurs d'entrainement les plus proches.

Ces vecteurs \`a proximit\'e sont essentiels pour l'estimation de l'hyperplan, nous les appelonts \textit{vecteur support}.

\paragraph{Calcul de marge}

Prenons un vecteur $\vec x_k$, nous pouvons calculer une distance par rapport \`a un plan avec $h_{w,b}$.

\begin{equation}
	h_{w, b} = \frac{l_k(\vec w^T\vec x_k + b)}{\lVert \vec w \rVert}
\end{equation}

where $\lVert \vec w \rVert$ is the euclidian distance, the general formula regardless of the size of the dimensions ($N$) is given by:
\begin{equation}
	\lVert \vec w \rVert = \sqrt{\sum_{i=1}^N x_i^2}
\end{equation}

The margin of an hyperplane is the smallest distance to one of the support vector and it's obtain by computing the following formula:

\begin{equation}
	\min_k \frac{l_k(\vec w^T\vec x_k + b)}{\lVert \vec w \rVert}
\end{equation}

Since we're looking to maximize margin, \textit{find the hyperplane with the largest margin}, we'll keep the maximum value in functions of $w$ and $b$ with the following equation:

\begin{equation}
	\arg\max_{w,b}\min_k \frac{l_k(\vec w^T\vec x_k + b)}{\lVert \vec w \rVert}
\end{equation}

\begin{equation}
	\min_{w,b,\xi} \frac{1}{2} \lVert \vec w \rVert ^2 + C\sum_{i=1}^m \xi_i^p
\end{equation}


\section{Kernel trick}

Voici une liste non-exhaustive de diff\'erents noyaux \`a notre disposition :
\begin{description}
	\item[Polynomial] $K(x, x\prime) = (\alpha x^T x\prime + \lambda )^d$
	\item[Gaussien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime \rVert^2}{2\sigma^2})$
	\item[Laplacien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime\rVert}{\sigma}$
	\item[Rationnel] $K(x, x\prime) = 1 - \frac{\lVert x\prime - x \rVert^2}{\lVert x\prime - x \rVert^2 + \sigma}$
\end{description}


\section{Python implementation}

\subsection{Classification}

\inputminted{python}{code/machine-learning/svc.py}

\subsection{Regression}

\inputminted{python}{code/machine-learning/svr.py}

