\gls{SVM} est un algorithme de \textit{machine learning} utilisé pour la classification et la prédiction de données\footnote{Nous dénoterons cependant une utilisation plus fréquente dans des problèmes de classification.}. 
Dans ce chapitre nous verrons le fonctionnement de cet algorithme dans le cadre d'une classification linéairement séparable et non-linéairement séparable.

\paragraph{}
The main concept in \gls{SVM} is based on finding an hyperplane $h$ optimized to categorize new examples.
In other terms, \gls{SVM} goal is to find an hyperplane whose distance to the nearest element of each tag is the largest.

\paragraph{Applications}
\gls{SVM}s can be used to solde various real-worl problems:
\begin{itemize}
	\item \gls{SVM}s are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.
	\item Classification of images
	\item Classification of satellite data
	\item Hand-written characters can be recognized using SVM
	\item Proteins classification
\end{itemize}

\section{Introduction}

In this case, we will focus on binary classification problems.
Let's consider a weight vector $\mathbf w = \langle \begin{matrix} w_1 & \dots & w_N \end{matrix} \rangle^T$, an entry vector $\mathbf x = \langle \begin{matrix} x_1 & \dots & x_N \end{matrix} \rangle^T$ and an biais $b$ which is basically just a real number.
From this equation, we can determine whether or not a new entry vector $\mathbf x$ belongs to which class.
If $h(\mathbf x) \geq 0$ then $\mathbf x$ is marked as positive and vice versa:

\begin{equation}
	h(\mathbf x) = \mathbf w^T\mathbf x + b \begin{cases}h(\mathbf x)\geq 0\to \text{class 1} \\h(\mathbf x)< 0\to \text{class 2}\end{cases}
\end{equation}

\subsection{Train SVMs}

An hyperplane try to maximize margin, in other terms, the minimal distance between training vectors and hyperplane.
Thoses training vectors located at this minimal distance are called \textit{support vectors}.

\paragraph{Compute margin}

Given a vector $\mathbf x_k$, we can compute its distance with $h_{w,b}$ :

\begin{equation}
	\frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

where $\lVert \mathbf w \rVert$ is the euclidian distance, the general formula regardless of the size of the dimensions ($N$) is given by:
\begin{equation}
	\lVert \mathbf w \rVert = \sqrt{\sum_{i=1}^N x_i^2}
\end{equation}

The margin of an hyperplane is the smallest distance to one of the support vector and it's obtain by computing the following formula:

\begin{equation}
	\min_k \frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

Since we're looking to maximize margin, \textit{find the hyperplane with the largest margin}, we'll keep the maximum value in functions of $w$ and $b$ with the following equation:

\begin{equation}
	\arg\max_{w,b}\min_k \frac{l_k(\mathbf w^T\mathbf x_k + b)}{\lVert \mathbf w \rVert}
\end{equation}

\begin{equation}
	\min_{w,b,\xi} \frac{1}{2} \lVert \mathbf w \rVert ^2 + C\sum_{i=1}^m \xi_i^p
\end{equation}


\section{Kernel trick}

Here is a non-exhaustive list of different kernels available to our case:
\begin{description}
	\item[Polynomial] $K(x, x\prime) = (\alpha x^T x\prime + \lambda )^d$
	\item[Gaussien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime \rVert^2}{2\sigma^2})$
	\item[Laplacien] $K(x, x\prime) = \exp(-\frac{\lVert x - x\prime\rVert}{\sigma}$
	\item[Rationnel] $K(x, x\prime) = 1 - \frac{\lVert x\prime - x \rVert^2}{\lVert x\prime - x \rVert^2 + \sigma}$
\end{description}


\section{Python implementation}

\subsection{Classification}

\inputminted{python}{code/machine-learning/svc.py}

\subsection{Regression}

\inputminted{python}{code/machine-learning/svr.py}

